{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M5.1.4 Dummy Variables.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uUlWOXTOtZkP","colab_type":"text"},"source":["<!-- dummy-coding: -->\n","# Dummy coding and the general linear model\n","\n","Before we start using GLMs we should address a problem that many of you will confront when dealing with categorical predictor variables. **It is important to remember that you cannot but categorical variables straight into a regression model without some form of transformation.** Two techniques are typically used and they are :\n","\n","* hot encoding the categorical variables or\n","* converting them to dummy variables.\n","\n","So imagine we are trying to build a model that predicts the sales of ice-cream based on purchasers age category. We have 3 categories and they are Child, Young adult and adult. So if we were to code them we might use the numbers $\\{1,2,3\\}$, This does not mean that people in category $1$ are 3 times younger than those in catgeory $3$. So this causes a problem because they are not really numbers but categories with no real scale. \n","\n","In order to use hot encoded variables we convert the category variable into 3 variables. $\\hat{\\beta_1}$ would be equal to 1 where the person is a child and $0$ else where, $\\hat{\\beta_2}$ would be equal to 1 where the person is a young adult and $0$ else where and $\\hat{\\beta_3}$ would be equal to 1 where the person is a adult and $0$ else where. \n","Now suppose we had 12 cases, 4 in each category then if we were to hot encode the age variable we would create a matrix such outlined below:\n","\n","\n","$$\\hat{X} =\n"," \\begin{pmatrix}\n","  1  & 0 & 0 \\\\\n","  1  & 0 & 0 \\\\\n","  1  & 0 & 0 \\\\\n","  1  & 0 & 0 \\\\\n","  0  & 1 & 0 \\\\\n","  0  & 1 & 0 \\\\\n","  0  & 1 & 0 \\\\\n","  0  & 1 & 0 \\\\\n","  0  & 0 & 1 \\\\\n","  0  & 0 & 1 \\\\\n","  0  & 0 & 1 \\\\\n","  0  & 0 & 1 \\\\\n"," \\end{pmatrix}$$\n","\n","Notice how each column in this matrix has a 1 or a 0 and each row only has only a single 1 signifying the age category of the case.\n","\n","Now we would use this matrix in our model to transform our original model:\n","$$Y_i=\\alpha + \\beta_1. X_1 + \\epsilon_i$$\n","\n","to \n","\n","$$Y_i=\\hat{\\beta_1}. \\hat{X_1} +\\hat{\\beta_2}.\\hat{X_2}+\\hat{\\beta_3}. \\hat{X_3}+ \\epsilon_i ~~~~ (1)$$\n","\n","</br>\n","\n","if we were to use a dummy variable approach we are $\\hat{X}$ matrix would be as follows:\n","\n","$$\\hat{X} =\n"," \\begin{pmatrix}\n","  1  & 0  \\\\\n","  1  & 0 \\\\\n","  1  & 0 \\\\\n","  1  & 0 \\\\\n","  0  & 1 \\\\\n","  0  & 1 \\\\\n","  0  & 1 \\\\\n","  0  & 1 \\\\\n","  0  & 0 \\\\\n","  0  & 0 \\\\\n","  0  & 0 \\\\\n","  0  & 0 \\\\\n"," \\end{pmatrix}$$\n","\n","The model would look like that shown in equation 2:\n","\n"," $$Y_i=\\alpha +\\hat{\\beta_1}.\\hat{X_1}+\\hat{\\beta_2}. \\hat{X_2}+ \\epsilon_i ~~~~(2)$$\n","\n","The difference between the 2 equations is subtle but should not be ignored. In equation (1) we assume there is no intercept, thus the interpretation is that you have a variable intercept or a different intercept for each age category. In this case the $\\beta 's$ are the average sales for each group. If you use the dummy variable approach you the $\\alpha$ value is the equivalent to the effect of Adults and the remaining $\\beta$ parameters are the ratio between each of the other age categories and Adults. They are not the effect of each age catgory on sales. Now the method you use depends on the effect you want to study but do not include an $\\alpha$ with a hotencoded variable as you will induce multi-collinearity. If you are using Scikit-learn's linear regression model or stats-model you should specify if you want an intercept or not. For one-hot encoding, always set fit_intercept=False, in sklearn. For dummy encoding, fit_intercept should always be set to True? I do not see any \"warning\" on the website.\n","\n","As usual leave your thoughts on the comments board.\n","\n"]}]}