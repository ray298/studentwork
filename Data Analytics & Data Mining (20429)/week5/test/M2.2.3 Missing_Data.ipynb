{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZAfSIckRmCt1"},"source":["# **Missing Data**\n","\n","Missing values can occur for a number of reasons such as operator error, faulty device, respondent refuses to enter the data or\n","They may be values, attributes entire records or entire sections. Typically, in any data analysis project we will find that there are missing values in our data. Many people believe it is ok to just insert the average value of the non missing values as their estimate for the missing values.\n","\n","</br>It might be worth while  to discuss amongst yourselves why this is not such a good idea.\n","\n","</br> Frankly, this is the one thing that every newcomer to data analytics does. My suspicion is you will do the same.  Lets look at some data. In it we have a small group of subjects who had their weight measured before and after a prescribed diet. Now you will see in the code below that the average weight for both groups increases at the end of the trial.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"PuGVupS6ourF","outputId":"72332ed4-d0b9-4960-ccb3-df52fbeadb03","executionInfo":{"status":"ok","timestamp":1739283587880,"user_tz":0,"elapsed":1918,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","import pandas as pd\n","from scipy import stats\n","data={'Name':['jane','Tom', 'nick', 'krish', 'jack'],\n","      'Weight_before':[185,201, 211, 191, 180],\n","      'Weight_after':[189,204, 219, 195, 187]}\n","df= pd.DataFrame(data)\n","print(df.describe())\n","#print(df.groupby('Name').describe())\n","#print(df.groupby(['Time']).describe())\n","stats.ttest_rel(df['Weight_before'], df['Weight_after'])"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["       Weight_before  Weight_after\n","count       5.000000      5.000000\n","mean      193.600000    198.800000\n","std        12.481987     13.084342\n","min       180.000000    187.000000\n","25%       185.000000    189.000000\n","50%       191.000000    195.000000\n","75%       201.000000    204.000000\n","max       211.000000    219.000000\n"]},{"output_type":"execute_result","data":{"text/plain":["TtestResult(statistic=-5.363390480545726, pvalue=0.00583306064524346, df=4)"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Z82IzwQyo2Bj"},"source":["Now you can see that the average difference is 5.36lbs and the the level of significance is 0.005 implying there is actually a significant difference between the before and after groups. We have used a paired t-test here. You can find out more about it [here](https://www.statisticssolutions.com/manova-analysis-paired-sample-t-test/) Now lets say nick and Jacks after weights were missing and we replaced them with the average weight from the Weight_after column. If you re run the code with the the new imputed values then you will find that there is no significant difference. We would possibly draw the wrong conclusions.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"l0aRchGo4gj1","outputId":"dd175ed0-948e-4513-e344-ee72cb39af1c","executionInfo":{"status":"ok","timestamp":1739283648196,"user_tz":0,"elapsed":5183,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","import pandas as pd\n","from scipy import stats\n","#data={'Name':['jane','Tom', 'nick', 'krish', 'jack'],\n","#      'Weight_before':[185,201, 211, 191, 180],\n","#      'Weight_after':[189,204, None, 195, None]}\n","data={'Name':['jane','Tom', 'nick', 'krish', 'jack'],\n","      'Weight_before':[185,201, 211, 191, 180],\n","      'Weight_after':[189,204,196.000000 , 195, 196.000000]}\n","df= pd.DataFrame(data)\n","print(df.describe())\n","#print(df.groupby('Name').describe())\n","#print(df.groupby(['Time']).describe())\n","\n","stats.ttest_rel(df['Weight_before'], df['Weight_after'])"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["       Weight_before  Weight_after\n","count       5.000000      5.000000\n","mean      193.600000    196.000000\n","std        12.481987      5.338539\n","min       180.000000    189.000000\n","25%       185.000000    195.000000\n","50%       191.000000    196.000000\n","75%       201.000000    196.000000\n","max       211.000000    204.000000\n"]},{"output_type":"execute_result","data":{"text/plain":["TtestResult(statistic=-0.4832976746641416, pvalue=0.6541475396789264, df=4)"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"oX-brUC6_PR7"},"source":["#Detecting Missing Data\n","It is always worth completing a pandas describe on your dataset if you can. This might detect some overtly missing data. Other ways are as follows:\n","\n","* Match data specifications against data - are all attributes present?\n","\n","* Scan individual records-are there gaps?\n","\n","* Rough checks: number of files, files sizes, number of records and the number of duplicates,\n","\n","* Compare estimate (averages, frequencies, medians) with expected values and boundaries.\n","\n","* values are truncated or censored, check for spikes\n","\n","* Missing values and defaults are indistinguishable-too many missing values? Ask an expert?\n","\n","* Errors of omission. Eg all calls missing from a specific area.\n","\n","\n"]}]}