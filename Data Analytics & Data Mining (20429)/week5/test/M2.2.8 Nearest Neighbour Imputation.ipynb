{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZrKDNHsK2zoE"},"source":["# **Nearest neighbours imputation**\n","\n","Skikit-learn also give you the KNNimputer which is a multivariate solution that effectively works on the rows of data rather than the columns. The KNN stands for K nearest neigthbors and the algorithm effectively finds the rows with non missing values that are closest to the row and the missing value. The values of the non missing neighbors are then used to create the estimate of the missing value. There are various approachs to this estimate and sjikit learn outlines it as follows:\n","\n","\"When the number of available neighbors is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform. For more information on the methodology\"\n","\n","Lets read the dataset from the previous example in and run the algorithm."]},{"cell_type":"code","metadata":{"id":"wk-UeKq_4NzH","outputId":"741a522d-c98d-4abd-877c-63dc25e4a5f6","executionInfo":{"status":"ok","timestamp":1707136119319,"user_tz":0,"elapsed":400,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import KNNImputer\n","y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n","    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n","    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n","    [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]])\n","\n","#y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n","#    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n","#    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000]])\n","\n","#y=np.reshape(y, (4, 16))\n","y=y.transpose()\n","#random_state=0\n","\n","imputer = KNNImputer(n_neighbors=2,weights=\"distance\")\n","y_res=imputer.fit_transform(y)\n","# the model learns that the second feature is double the first\n","print(y_res)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[[7.80000000e+02 5.10000000e+00 7.80000000e+04 5.00000000e-01]\n"," [7.50000000e+02 4.50000000e+00 7.50000000e+04 5.50000000e-01]\n"," [6.90000000e+02 7.07659925e+00 1.00000000e+05 1.00000000e-01]\n"," [7.10000000e+02 3.30000000e+00 7.10000000e+04 6.00000000e-01]\n"," [6.80000000e+02 3.60000000e+00 6.80000000e+04 7.00000000e-01]\n"," [7.30000000e+02 9.30000000e+00 7.00000000e+04 4.50000000e-01]\n"," [6.90000000e+02 6.70000000e+00 6.90000000e+04 5.60000000e-01]\n"," [7.20000000e+02 2.80000000e+00 7.20000000e+04 7.30000000e-01]\n"," [7.40000000e+02 5.40000000e+00 7.40000000e+04 4.50000000e-01]\n"," [9.00000000e+02 7.14595202e+00 6.90000000e+04 6.70000000e-01]\n"," [9.50000000e+02 7.80000000e+00 1.02000000e+05 4.30000000e-01]\n"," [9.75000000e+02 6.73718709e+00 1.01000000e+05 2.30000000e-01]\n"," [9.95000000e+02 4.97799832e+00 7.90000000e+04 7.80000000e-01]\n"," [1.00000000e+03 1.01000000e+01 1.14000000e+05 4.20000000e-01]\n"," [1.01000000e+03 6.70000000e+00 1.01000000e+05 3.60000000e-01]\n"," [1.02000000e+03 7.20767902e+00 9.50000000e+04 2.30000000e-01]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"2JPL5g7v9TRU"},"source":["Again experiment with your results. When I say experiment run the regression code again so you can see the impact of answers. Do you think normalization of the variables would be wise? Try it out and see what happens. Should we drop the y variable from the KNN approach?"]}]}