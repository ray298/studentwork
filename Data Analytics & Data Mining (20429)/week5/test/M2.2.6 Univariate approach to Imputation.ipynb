{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2kji7t3IawzG"},"source":["# **Univariate approach to Imputation**\n","The next 3 steps are taken from the [skikit learn repository](https://scikit-learn.org/stable/modules/impute.html). The first of these is a simple tool to help you implement Univariate imputation. Remember, it is always wise to do this with MCAR variables. Ski-learns SimpleImputer has options to use a constant, mean, median or mode for your missing values. It is important before you implement this process to visualise the data. For example when you have normally distributed variable you should probably use the mean. If you have outliers or skewed data use the median or the mode. The code in the next section gives a simple example of this. I would strongly recommend that you experiment with it so you can understand the implications of your choice.\n"]},{"cell_type":"code","metadata":{"id":"0UNedeegc6eJ","outputId":"6e169f40-a71e-433e-b4a9-4110c97f2e7a","executionInfo":{"status":"ok","timestamp":1739284167733,"user_tz":0,"elapsed":12314,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import numpy as np\n","from sklearn.impute import SimpleImputer\n","\n","y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n","    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n","    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n","    [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]])\n","#y=np.reshape(y, (4, 16))\n","y=y.transpose()\n","#print(y)\n","imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n","#imp.fit(y)\n","y=imp.fit_transform(y)\n","print(y)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[[7.80000000e+02 5.10000000e+00 7.80000000e+04 5.00000000e-01]\n"," [7.50000000e+02 4.50000000e+00 7.50000000e+04 5.50000000e-01]\n"," [6.90000000e+02 5.93636364e+00 1.00000000e+05 1.00000000e-01]\n"," [7.10000000e+02 3.30000000e+00 7.10000000e+04 6.00000000e-01]\n"," [6.80000000e+02 3.60000000e+00 6.80000000e+04 7.00000000e-01]\n"," [7.30000000e+02 9.30000000e+00 7.00000000e+04 4.50000000e-01]\n"," [6.90000000e+02 6.70000000e+00 6.90000000e+04 5.60000000e-01]\n"," [7.20000000e+02 2.80000000e+00 7.20000000e+04 7.30000000e-01]\n"," [7.40000000e+02 5.40000000e+00 7.40000000e+04 4.50000000e-01]\n"," [9.00000000e+02 5.93636364e+00 6.90000000e+04 6.70000000e-01]\n"," [9.50000000e+02 7.80000000e+00 1.02000000e+05 4.30000000e-01]\n"," [9.75000000e+02 5.93636364e+00 1.01000000e+05 2.30000000e-01]\n"," [9.95000000e+02 5.93636364e+00 7.90000000e+04 7.80000000e-01]\n"," [1.00000000e+03 1.01000000e+01 1.14000000e+05 4.20000000e-01]\n"," [1.01000000e+03 6.70000000e+00 1.01000000e+05 3.60000000e-01]\n"," [1.02000000e+03 5.93636364e+00 9.50000000e+04 2.30000000e-01]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"iikjo9rJk9Z1"},"source":["We have to transform the matrix for SimpleImputer to handle it.\n","Now you will notice the missing values that were in the second column have not been converted to 5.9363. We now implement a simple regression model and note the results."]},{"cell_type":"code","metadata":{"id":"CeMlnpLvjbZa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739284189328,"user_tz":0,"elapsed":54,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"outputId":"c5aec051-0965-4b01-a8a4-cbf30143c9b6"},"source":["import statsmodels.formula.api as sm\n","import statsmodels.stats.stattools as st\n","import statsmodels.stats.api as sms\n","import pandas as pd\n","df=pd.DataFrame(y)\n","df.columns=['X1','X2','X3','Y']\n","\n","formula_str=\"Y~X1 + X2 +X3\"\n","\n","result=sm.ols(formula=formula_str,data=df).fit()\n","print(result.summary())"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      Y   R-squared:                       0.597\n","Model:                            OLS   Adj. R-squared:                  0.497\n","Method:                 Least Squares   F-statistic:                     5.931\n","Date:                Tue, 11 Feb 2025   Prob (F-statistic):             0.0101\n","Time:                        14:29:50   Log-Likelihood:                 11.472\n","No. Observations:                  16   AIC:                            -14.94\n","Df Residuals:                      12   BIC:                            -11.85\n","Df Model:                           3                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","Intercept      0.9979      0.226      4.413      0.001       0.505       1.491\n","X1             0.0006      0.000      1.778      0.101      -0.000       0.001\n","X2            -0.0074      0.021     -0.347      0.734      -0.054       0.039\n","X3         -1.187e-05   3.26e-06     -3.641      0.003    -1.9e-05   -4.76e-06\n","==============================================================================\n","Omnibus:                        0.029   Durbin-Watson:                   2.008\n","Prob(Omnibus):                  0.986   Jarque-Bera (JB):                0.249\n","Skew:                           0.028   Prob(JB):                        0.883\n","Kurtosis:                       2.392   Cond. No.                     5.64e+05\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 5.64e+05. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=16\n","  res = hypotest_fun_out(*samples, **kwds)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y26hNKg_moz8"},"source":["Now we will re-run the analysis but this time with the median as our estimate of the missing value."]},{"cell_type":"code","metadata":{"id":"jSC8WCLNiw18","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739284218285,"user_tz":0,"elapsed":35,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"outputId":"3231aacf-785f-4fa5-9d8e-7dc3fe55d426"},"source":["y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n","    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n","    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n","    [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]])\n","\n","y=y.transpose()\n","imp = SimpleImputer(missing_values=np.nan, strategy='median')\n","#imp.fit(y)\n","y=imp.fit_transform(y)\n","print(y)\n","df=pd.DataFrame(y)\n","df.columns=['X1','X2','X3','Y']\n","\n","formula_str=\"Y~X1 + X2 +X3\"\n","\n","result=sm.ols(formula=formula_str,data=df).fit()\n","print(result.summary())"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[[7.80e+02 5.10e+00 7.80e+04 5.00e-01]\n"," [7.50e+02 4.50e+00 7.50e+04 5.50e-01]\n"," [6.90e+02 5.40e+00 1.00e+05 1.00e-01]\n"," [7.10e+02 3.30e+00 7.10e+04 6.00e-01]\n"," [6.80e+02 3.60e+00 6.80e+04 7.00e-01]\n"," [7.30e+02 9.30e+00 7.00e+04 4.50e-01]\n"," [6.90e+02 6.70e+00 6.90e+04 5.60e-01]\n"," [7.20e+02 2.80e+00 7.20e+04 7.30e-01]\n"," [7.40e+02 5.40e+00 7.40e+04 4.50e-01]\n"," [9.00e+02 5.40e+00 6.90e+04 6.70e-01]\n"," [9.50e+02 7.80e+00 1.02e+05 4.30e-01]\n"," [9.75e+02 5.40e+00 1.01e+05 2.30e-01]\n"," [9.95e+02 5.40e+00 7.90e+04 7.80e-01]\n"," [1.00e+03 1.01e+01 1.14e+05 4.20e-01]\n"," [1.01e+03 6.70e+00 1.01e+05 3.60e-01]\n"," [1.02e+03 5.40e+00 9.50e+04 2.30e-01]]\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      Y   R-squared:                       0.594\n","Model:                            OLS   Adj. R-squared:                  0.492\n","Method:                 Least Squares   F-statistic:                     5.846\n","Date:                Tue, 11 Feb 2025   Prob (F-statistic):             0.0106\n","Time:                        14:30:19   Log-Likelihood:                 11.402\n","No. Observations:                  16   AIC:                            -14.80\n","Df Residuals:                      12   BIC:                            -11.71\n","Df Model:                           3                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","Intercept      1.0019      0.227      4.418      0.001       0.508       1.496\n","X1             0.0006      0.000      1.742      0.107      -0.000       0.001\n","X2            -0.0026      0.021     -0.128      0.900      -0.047       0.042\n","X3         -1.211e-05   3.27e-06     -3.705      0.003   -1.92e-05   -4.99e-06\n","==============================================================================\n","Omnibus:                        0.177   Durbin-Watson:                   2.055\n","Prob(Omnibus):                  0.915   Jarque-Bera (JB):                0.383\n","Skew:                           0.023   Prob(JB):                        0.826\n","Kurtosis:                       2.243   Cond. No.                     5.63e+05\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 5.63e+05. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=16\n","  res = hypotest_fun_out(*samples, **kwds)\n"]}]},{"cell_type":"markdown","metadata":{"id":"0_tWG-3aiMcG"},"source":["There isn't much difference between the results. </br></br>\n","\n","Insert a number of high results into the experience variable and re-run your analysis. What happens?"]}]}