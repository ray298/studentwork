{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGygf9tyz8mS",
        "outputId": "3d320f81-0da8-4a1a-8294-c9af1f9f241c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Ed_standard         Salary  Work_experience  happiness    Missing\n",
            "count    16.000000      16.000000        11.000000   16.00000  16.000000\n",
            "mean    833.750000   83625.000000         5.936364    0.48500   0.312500\n",
            "std     136.607711   15564.382416         2.420443    0.19225   0.478714\n",
            "min     680.000000   68000.000000         2.800000    0.10000   0.000000\n",
            "25%     717.500000   70750.000000         4.050000    0.40500   0.000000\n",
            "50%     765.000000   76500.000000         5.400000    0.47500   0.000000\n",
            "75%     980.000000  100250.000000         7.250000    0.61750   1.000000\n",
            "max    1020.000000  114000.000000        10.100000    0.78000   1.000000\n",
            "Optimization terminated successfully.\n",
            "         Current function value: 0.527167\n",
            "         Iterations 6\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:                Missing   No. Observations:                   16\n",
            "Model:                          Logit   Df Residuals:                       13\n",
            "Method:                           MLE   Df Model:                            2\n",
            "Date:                Tue, 18 Feb 2025   Pseudo R-squ.:                  0.1512\n",
            "Time:                        11:37:09   Log-Likelihood:                -8.4347\n",
            "converged:                       True   LL-Null:                       -9.9374\n",
            "Covariance Type:            nonrobust   LLR p-value:                    0.2225\n",
            "===============================================================================\n",
            "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
            "-------------------------------------------------------------------------------\n",
            "const          -6.8208      4.189     -1.628      0.103     -15.031       1.390\n",
            "Ed_standard     0.0089      0.007      1.335      0.182      -0.004       0.022\n",
            "Salary      -1.874e-05   5.46e-05     -0.343      0.731      -0.000    8.83e-05\n",
            "===============================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df3 = pd.read_csv('class-grades.csv')\n",
        "print(df3.describe())\n",
        "\n",
        "\n",
        "df3.loc[df3['Final'].isnull()==True,'Missing']=\"Y\"\n",
        "df3.loc[df3['Final'].isnull()==False,'Missing']=\"N\"\n",
        "print(df3.loc[df3['Missing']==\"Y\",'Missing'])\n",
        "\n",
        "\n",
        "#dfassign=df3.pivot(columns = 'Missing',values=['Assignment'])['Assignment']\n",
        "#print(dfassign['Y'])\n",
        "\n",
        "#dfassign.boxplot(column=['Y','N'],grid=False)\n",
        "\n",
        "#将 Midterm 按 Missing（Final 是否缺失）分类：\n",
        "dfassign=df3.pivot(columns = 'Missing',values=['Midterm'])['Midterm']\n",
        "#print(dfassign['Y'])\n",
        "\n",
        "#目的是查看 Midterm 是否影响 Final 的缺失情况。\n",
        "dfassign.boxplot(column=['Y','N'],grid=False)\n",
        "#d.boxplot(column=['A', 'B', 'C', 'D'], grid=False)\n",
        "#plt.show()\n",
        "\n",
        "\"\"\"\n",
        "三种缺失值（MCAR、MAR、MNAR）对应的插补方法\n",
        "1. 适用于 MCAR（完全随机缺失）\n",
        "MCAR（Missing Completely at Random）：缺失值与任何变量无关，数据的缺失是完全随机的。\n",
        "插补方法：\n",
        "均值/中位数/众数填充（Mean/Median/Mode Imputation）：简单且不会引入系统性偏差，适用于数值型或类别型数据。\n",
        "删除缺失数据（Listwise or Pairwise Deletion）：如果缺失值较少，直接删除不会影响数据的代表性。\n",
        "\n",
        "2. 适用于 MAR（随机缺失）\n",
        "MAR（Missing at Random）：缺失值与其他可观测变量相关，但与自身值无关。\n",
        "插补方法：\n",
        "回归插补（Regression Imputation）：利用其他变量预测缺失值，适用于数值型数据。\n",
        "多重插补（MICE）（Multiple Imputation by Chained Equations）：模拟多个插补数据集，并结合不确定性进行估计。\n",
        "\n",
        "3. 适用于 MNAR（非随机缺失）\n",
        "MNAR（Missing Not at Random）：缺失值与自身值相关，即使控制其他变量，仍然存在系统性缺失。\n",
        "插补方法：\n",
        "KNN 近邻插补（K-Nearest Neighbors Imputation）：基于最相似的样本进行插补，适用于数据模式明显的情况。\n",
        "最大似然估计（MLE）（Maximum Likelihood Estimation）：基于统计模型估计缺失值，适用于有明显分布假设的数据。\n",
        "使用外部数据或专家知识：由于 MNAR 可能隐藏重要信息，最好尽量获取额外数据，而不是简单插补。\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from statsmodels.api import add_constant\n",
        "import statsmodels.discrete.discrete_model as sml\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rc(\"font\", size=14)\n",
        "sns.set(style=\"white\")\n",
        "sns.set(style=\"whitegrid\", color_codes=True)\n",
        "\n",
        "candidates = {'Ed_standard': [780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n",
        "              'Work_experience':[5.1,4.5,None,3.3,3.6,9.3,6.7,2.8,5.4,None,7.8,None,None,10.1,6.7,None],\n",
        "              'Salary': [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n",
        "              'happiness': [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]\n",
        "              }\n",
        "df = pd.DataFrame(candidates,columns= ['Ed_standard','Salary','Work_experience','happiness'])\n",
        "\n",
        "df.loc[df['Work_experience'].isnull()==False,'Missing']=0\n",
        "df.loc[df['Work_experience'].isnull()==True,'Missing']=1\n",
        "\n",
        "print(df.describe())\n",
        "corr = df[['Ed_standard','Salary','Work_experience','happiness']].corr()\n",
        "corr.style.background_gradient(cmap='coolwarm')\n",
        "\n",
        "\n",
        "#df.to_csv('candidates.csv')#保存成 CSV 文件\n",
        "\n",
        "\n",
        "\n",
        "X=df[['Ed_standard','Salary']]\n",
        "X = add_constant(X)\n",
        "\n",
        "y=df['Missing']\n",
        "\n",
        "logit = sml.Logit(y, X).fit()\n",
        "print(logit.summary())  #LLR p-value（似然比检验的 p 值）= 0.2225，表示整体模型的统计显著性不强  R²（Pseudo R²）= 0.1512 这个值较低，说明 Ed_standard 和 Salary 对 Missing 的解释能力不强。\n",
        "#print(logit.predict())\n",
        "\n",
        "\"\"\"逻辑回归（Logistic Regression），用于分析 Work_experience 变量的缺失（Missing=1）是否与 Ed_standard（教育标准）和 Salary（薪资）相关。\n",
        "\n",
        "结论\n",
        "Work_experience 的缺失（Missing=1）与 Ed_standard 和 Salary 没有显著关系：\n",
        "\n",
        "逻辑回归分析表明，这两个变量的 p 值 > 0.05，无法预测 Work_experience 是否缺失。\n",
        "这表明 Work_experience 可能是 MCAR（完全随机缺失），而不是 MAR（随机缺失）。\n",
        "如果 Work_experience 是 MCAR，可以直接删除缺失值或使用均值填充：\n",
        "\n",
        "由于缺失值与 Ed_standard 和 Salary 没有关系，可以直接删除缺失数据或使用简单插补（如均值或中位数填充）。\n",
        "如果 Work_experience 影响后续分析，可以使用 KNN 或 MICE 进行插补：\n",
        "\n",
        "如果数据是 MAR，建议使用回归插补或 MICE（MICE - Multiple Imputation by Chained Equations），保持数据的关系结构。\n",
        "如果数据是 MNAR，建议使用KNN 或机器学习方法，避免插补带来的系统性偏差。。\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"简单插补 Univariate Imputation 适用于MCAR（完全随机缺失）\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n",
        "    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n",
        "    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n",
        "    [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]])\n",
        "#y=np.reshape(y, (4, 16))\n",
        "y=y.transpose()\n",
        "#填补np.nan，使用mean  strategy='median'\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "#imp.fit(y)\n",
        "y=imp.fit_transform(y)\n",
        "print(y)\n",
        "\n",
        "\n",
        "import statsmodels.formula.api as sm\n",
        "import statsmodels.stats.stattools as st\n",
        "import statsmodels.stats.api as sms\n",
        "import pandas as pd\n",
        "df=pd.DataFrame(y)\n",
        "df.columns=['X1','X2','X3','Y']\n",
        "\n",
        "formula_str=\"Y~X1 + X2 +X3\"\n",
        "\n",
        "result=sm.ols(formula=formula_str,data=df).fit()\n",
        "print(result.summary())\n",
        "\n",
        "\"\"\"观察填补 X2 缺失值后，看回归模型的系数和显著性是否受到影响。\n",
        "如果 X2 影响很大，可能需要不同的填补策略，比如 median（中位数填补）或 KNN（最近邻填补）。\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"IterativeImputer迭代插补 属于多变量插补 Multivariate Imputation，使用贝叶斯岭回归（BayesianRidge）作为回归模型\n",
        "IterativeImputer迭代插补 是 Scikit-learn 实现的 MICE（MICE - Multiple Imputation by Chained Equations）变体 \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "#方法一，形状难看\n",
        "y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n",
        "    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n",
        "    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n",
        "    [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]])\n",
        "#y=np.reshape(y, (4, 16))\n",
        "y=y.transpose()\n",
        "#random_state=0\n",
        "imp = IterativeImputer(max_iter=100, sample_posterior=True,tol=0.000001)\n",
        "y=imp.fit_transform(y)\n",
        "print(y)\n",
        "\n",
        "#方法2，形状好看\n",
        "y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n",
        "    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n",
        "    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n",
        "    [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]])\n",
        "y = y.T  # 转置数据，使形状匹配\n",
        "# 使用 IterativeImputer 进行插补\n",
        "imp = IterativeImputer(max_iter=100, sample_posterior=True, tol=1e-6, random_state=0)\n",
        "y_imputed = imp.fit_transform(y)\n",
        "# 转换为 pandas DataFrame 并格式化显示\n",
        "df = pd.DataFrame(y_imputed, columns=[\"Feature_1\", \"Feature_2\", \"Feature_3\", \"Feature_4\"])\n",
        "print(df)\n",
        "\n",
        "\"\"\"IterativeImputer（迭代插补）是一种 多变量插补 方法，它使用 其他特征来预测缺失值，不像 SimpleImputer 仅使用均值或中位数。\n",
        "\n",
        "工作原理：使用贝叶斯岭回归（BayesianRidge）作为回归模型\n",
        "选择 一个 变量作为 y（需要填补的特征）。\n",
        "其他变量作为 X（用来预测 y）。\n",
        "用 X 训练一个回归模型来预测 y 中的 np.nan。\n",
        "对所有变量重复这个过程，迭代 max_iter 次，最终返回最优结果。\n",
        "\n",
        "让插补过程带有一定的随机性，每次运行都会略微不同。\n",
        "这样可以模拟数据的不确定性，但也意味着结果不稳定\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"KNNImputer（基于 K 近邻的插补）\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import KNNImputer\n",
        "y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n",
        "    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n",
        "    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n",
        "    [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]])\n",
        "\n",
        "#y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n",
        "#    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n",
        "#    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000]])\n",
        "\n",
        "#y=np.reshape(y, (4, 16))\n",
        "y=y.transpose()\n",
        "#random_state=0\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=2,weights=\"distance\")\n",
        "y_res=imputer.fit_transform(y)\n",
        "# the model learns that the second feature is double the first\n",
        "print(y_res)\n",
        "\n",
        "\n",
        "#在 KNNImputer 里，Y 也是一个特征变量，因此：如果 Y 也有缺失值，KNN 可能会错误地用 X1, X2, X3 来预测 Y，\n",
        "#去掉 Y 变量后插补\n",
        "X = y[:, :-1]  # 只使用 X1, X2, X3 进行插补\n",
        "imputer = KNNImputer(n_neighbors=2, weights=\"distance\")\n",
        "X_res = imputer.fit_transform(X)\n",
        "\n",
        "# 重新组合 X 和 Y\n",
        "y_res = np.hstack([X_res, y[:, -1].reshape(-1, 1)])\n",
        "print(y_res)\n",
        "\n",
        "\n",
        "\"\"\"KNN 插补后，重新回归分析\"\"\"\n",
        "import statsmodels.formula.api as sm\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(y_res, columns=['X1', 'X2', 'X3', 'Y'])  # 转换为 DataFrame\n",
        "\n",
        "formula_str = \"Y ~ X1 + X2 + X3\"\n",
        "result = sm.ols(formula=formula_str, data=df).fit()\n",
        "print(result.summary())\n",
        "\n",
        "#表明插入数据后更好了\n",
        "#Log-Likelihood = 12.260\t对数似然函数值提高了（之前是 11.472），说明拟合更好。\n",
        "#AIC = -16.52, BIC = -13.43\tAIC/BIC 变小，表明模型改进。\n",
        "#Prob(JB) = 0.987 残差分布接近正态\n",
        "#Durbin-Watson = 1.890\t接近 2，说明残差无明显自相关问题。\n",
        "\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "X = df[['X1', 'X2', 'X3']]\n",
        "X['Intercept'] = 1  # 添加常数项\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Outlier\"\"\"\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "housing = fetch_california_housing()  # 载入加州房价数据集\n",
        "print(housing.DESCR)  # 输出数据集描述\n",
        "\n",
        "\n",
        "x = housing.data  # 特征数据\n",
        "y = housing.target  # 房价\n",
        "\n",
        "columns = housing.feature_names  # 特征名称\n",
        "housing_df = pd.DataFrame(housing.data)  # 转换为 DataFrame\n",
        "\n",
        "housing_df.columns = columns  # 设置列名\n",
        "housing_df['Y'] = pd.Series(y)  # 添加目标变量（房价） 把 y 转换为 pandas Series，并添加到 DataFrame\n",
        "housing_df\n",
        "\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "sns.boxplot(x=housing_df['AveOccup']) #boxplot看Outlier  这是 look for univariate outliers 的方法\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "x = np.array(housing_df.index.tolist())  # X轴：索引值      将 DataFrame 的索引转换为 Python list，然后再转为 NumPy array。\n",
        "y1 = np.array(housing_df['AveOccup'])  # Y轴：每户平均居住人数  默认是 pandas.Series，需要转换为 NumPy array 以便 matplotlib 正确处理。\n",
        "\n",
        "f = plt.figure()  # 创建画布\n",
        "ax = f.add_subplot(111)  # 添加子图\n",
        "\n",
        "plt.plot(x, y1)  # 绘制折线图\n",
        "plt.axhline(y=housing_df['AveOccup'].mean())  # 添加均值线\n",
        "plt.axhline(y=housing_df['AveOccup'].mean() + 3 * housing_df['AveOccup'].std(), color='r')  # 添加 +3σ 线\n",
        "plt.axhline(y=housing_df['AveOccup'].mean() - 3 * housing_df['AveOccup'].std(), color='r')  # 添加 -3σ 线\n",
        "\n",
        "plt.title('Individual Charts', fontsize=8)  # 设置标题\n",
        "plt.show()  # 显示图表\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" Multivariate outlier\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.spatial import distance\n",
        "from scipy.stats import f\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# 加载加州房价数据集\n",
        "housing = fetch_california_housing()\n",
        "data = housing.data  # 特征数据\n",
        "target = housing.target  # 目标变量（房价中位数）\n",
        "\n",
        "# 将数据转换为 Pandas DataFrame\n",
        "housing_df = pd.DataFrame(data, columns=housing.feature_names)\n",
        "\n",
        "# 计算数据的行数（样本数）和列数（变量数）\n",
        "n = len(housing_df)  # 样本数量\n",
        "k = len(housing_df.columns)  # 变量数量\n",
        "\n",
        "# 计算 Hotelling's T² 统计量的 99% 临界值  #Hotelling's T² 统计量用于检测多变量数据中的异常点。\n",
        "Hotvalue = f.ppf([0.99], n, k) * (n * k) / ((n - k) + 1)\n",
        "print(\"Hotelling's T² 临界值:\", Hotvalue) # critical value（38.88768082）  Hotvalue是一个数组，而不是一个单一数值\n",
        "\n",
        "# 计算数据的逆协方差矩阵（用于 Mahalanobis 距离计算）\n",
        "inverse_cov = np.linalg.pinv(housing_df.cov().values)\n",
        "\n",
        "# 计算数据的均值向量\n",
        "xbar = housing_df.mean().values\n",
        "\n",
        "# 计算每个数据点的 Hotelling's T² 统计量    和inverse_cov一起体现多变量\n",
        "hotelling_scores = []\n",
        "for i in range(n):\n",
        "    mahalanobis_dist = distance.mahalanobis(housing_df.iloc[i], xbar, inverse_cov) #Mahalanobis 距离 是一种 多变量尺度（Multivariate Distance）\n",
        "    t2_value = (n * k) * mahalanobis_dist ** 2 / (k * (n - k))\n",
        "    hotelling_scores.append(t2_value)\n",
        "\n",
        "# 将 Hotelling's T² 统计量添加到 DataFrame\n",
        "housing_df['hotelling'] = hotelling_scores\n",
        "housing_df['critical value'] = Hotvalue[0] #如果 hotelling（T² 值） > critical value（38.88768082），则该点可能是异常值。\n",
        "\n",
        "# 绘制 Hotelling's T² 统计量的可视化图表\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(n), hotelling_scores, label='Hotelling T² Values')\n",
        "plt.axhline(y=housing_df['hotelling'].mean(), color='g', linestyle='--', label='Mean T²')\n",
        "plt.axhline(y=Hotvalue, color='r', linestyle='-', label='99% Critical Value')\n",
        "plt.title('Multivariate Outlier Detection Using Hotelling’s T² Test')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Hotelling’s T² Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 输出异常值（超出临界值的数据点）\n",
        "outliers = housing_df[housing_df['hotelling'] > housing_df['critical value']]\n",
        "print(\"异常值:\")\n",
        "print(outliers)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"分箱  Binning Methods for Data Smoothing\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 加载 Iris 数据集\n",
        "dataset = load_iris()\n",
        "data = dataset.data  # 取所有数值特征\n",
        "feature_names = dataset.feature_names  # 特征名称\n",
        "df = pd.DataFrame(data, columns=feature_names)\n",
        "\n",
        "# 选择某个特征列（例如：sepal width (cm)）\n",
        "col = df.iloc[:, 1]  # 选择第二列特征（花萼宽度）\n",
        "\n",
        "# 按照分箱数量排序\n",
        "sorted_col = np.sort(col.values)\n",
        "\n",
        "# 设定分箱数量（如 30 组，每组 5 个数据点）\n",
        "num_bins = 30\n",
        "bin_size = len(sorted_col) // num_bins  # 每个 bin 的大小\n",
        "\n",
        "# **1. 均值分箱**\n",
        "bin_means = sorted_col.copy()\n",
        "for i in range(0, len(sorted_col), bin_size):\n",
        "    bin_means[i:i+bin_size] = np.mean(sorted_col[i:i+bin_size])\n",
        "\n",
        "print(\"Bin Mean Smoothing:\\n\", bin_means)\n",
        "\n",
        "# **2. 边界分箱**\n",
        "bin_boundaries = sorted_col.copy()\n",
        "for i in range(0, len(sorted_col), bin_size):\n",
        "    min_val = sorted_col[i]\n",
        "    max_val = sorted_col[i+bin_size-1] if i+bin_size-1 < len(sorted_col) else sorted_col[-1]\n",
        "    for j in range(bin_size):\n",
        "        if abs(sorted_col[i+j] - min_val) < abs(sorted_col[i+j] - max_val):\n",
        "            bin_boundaries[i+j] = min_val\n",
        "        else:\n",
        "            bin_boundaries[i+j] = max_val\n",
        "\n",
        "print(\"Bin Boundary Smoothing:\\n\", bin_boundaries)\n",
        "\n",
        "# **3. 中位数分箱**\n",
        "bin_medians = sorted_col.copy()\n",
        "for i in range(0, len(sorted_col), bin_size):\n",
        "    median_val = np.median(sorted_col[i:i+bin_size])\n",
        "    bin_medians[i:i+bin_size] = median_val\n",
        "\n",
        "print(\"Bin Median Smoothing:\\n\", bin_medians)\n",
        "\n",
        "\n"
      ]
    }
  ]
}