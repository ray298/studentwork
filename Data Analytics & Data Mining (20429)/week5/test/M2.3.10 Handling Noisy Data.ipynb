{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M2.3.10 Handling Noisy Data.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XIbAZ6ofaCmb","colab_type":"text"},"source":["# **Noisy Data**\n","\n","Noise can generally be considered to be random error or variance in a measured variable. It can generally come from the following situations:\n","\n","* faulty data collection intruments\n","* data entry problems\n","* data transmission problems\n","* technology limitations\n","* inconsistency in naming convention\n","\n","Other data problems which require data cleaning are:\n"," * duplicate records\n"," * incomplete data\n"," * inconsistent data\n","\n","\n","Can you think of an example that gives us Noisy data?\n"]},{"cell_type":"markdown","metadata":{"id":"aXUcowtCbbfa","colab_type":"text"},"source":["Well you come across many in our daily lives. We have all been effected by bad mobile signals or fuzzy images on our TVs. These can come from interference or just plain old inconsistent data collection. \n","\n","So how do we handle noisy data?\n","\n","There are numerous techniques and as usual there is no eaxt answer. What we need to make sure is that we do not over process our data to the extent that it will remove inherent trends or cyclical patterns, but if there is a to much noise in our data then we will be unable make realistic predictions. So in effect we want to be able to do enough that is useful. In addition, we may find noise in our data that is caused by some outside force/variable that we have not measured. One should also be careful to differeniate between noise and outliers. Outliers are relatively rare occurences and noise can be systemic. Many commentators online fail in my opinion to differentiate between the 2, and treat them with the same processes. \n","\n","The following techniques are often used to handle Noisy Data:\n","\n","* Binning methods\n","> * Smooth by bin means, bin median or by bin boundaries\n","\n","* Clustering\n","> * detect and remove outliers?\n","\n","* Combined computer and human inspection\n","> * detect suspicious values and check by human\n","\n","* Regression\n","> smooth by fitting the data into regression functions\n","\n","* Moving Average/Exponential Smoothing etc.\n","> * Smooth time series values to romove inherint noise\n","\n","* Wavelett Analysis/Fourier Transforms\n","> * Creates new variables from alternate frequencies.\n","\n","In the next 2 topics we will look at some Discretization methods and a time series techniques such as Moving Average and Exponential smoothing. There are many smoothing techniques and they designed to detect trends in the presence of noisy data in cases in which the shape of the trend is unknown.  However, before you try any of these apporaches always exam the variables/features in question using simple graphs. They can tell alot about your data and can certainly in a time series case tell if there is a strong noise to [signal ratio](https://en.wikipedia.org/wiki/Signal-to-noise_ratio). Finally, if you smooth a variable and use this variable as predictor for your model then remember you are removing the variation from your analysis and can potentially induce bias. This will have potential impact on the range of outcomes. So you might find that you need to adapt your models to take account of this.\n","\n","The following links will give you a feel for how much smoothing is viable and also some of the techniques:\n","\n","* [Why smooth](https://rafalab.github.io/dsbook/smoothing.html)\n","\n","* [How much should we smooth?](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch04.pdf)\n","\n","Take a look at these links and try and get a feel for why we would smooth our data.\n"]}]}