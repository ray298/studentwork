{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"U7F4Ntvpm2Kd"},"source":["#**Lasso Regression**\n","Lasso Regression is an extension of Ridge regression, but has  one major benifit, and this is due to its abilty to directly help with feature reduction. \"The key difference between these techniques is that Lasso shrinks the less important featureâ€™s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features\", Toward Science)\n","\n","In ridge regression we had the following formula:\n","\n",">>$min_{\\beta}(y-X\\beta)^T(y-X\\beta) +\\lambda(\\beta^T\\beta-c)$ which gives\n","\n",">>$\\hat{B}_R={(X^TX+{\\lambda}I)}^{-1}X^Ty$\n","\n","Now for Lasso Regression the formula changes slightly, we replace the $\\lambda(\\beta^T\\beta-c)$ with $\\lambda(\\lvert\\beta\\rvert-c)$\n","\n","The impact of this change is that some of the parmeter estimates $\\beta$ will be set to zero, thus helping us with variable selection. There is a really nice explaination [here](https://stats.stackexchange.com/questions/74542/why-does-the-lasso-provide-variable-selection). In this explaination the authors explain that when the cost function is differentiated the part that has the absolute value drives the optimisation routine to select certain parameters as zero.\n","\n","#**Elastic Net**\n","Elastic Net is an ensemble of both the L1 and L2 regukraization techniques.\n","Generally, Lasso will eliminate many features, and reduce overfitting in your linear model. Ridge will reduce the impact of features that are not important in predicting your y values. Elastic Net combines feature elimination from Lasso and feature coefficient reduction from the Ridge model to improve your model's predictions.  We have completed a small example of it at the end of these notes.\n","\n","</br>\n","\n","Lets look at the data we had in the ridge regression step and see how it performs under lasso regression,\n","\n"]},{"cell_type":"code","metadata":{"id":"fOa2pkdqEZd1","executionInfo":{"status":"ok","timestamp":1739287101836,"user_tz":0,"elapsed":8,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}}},"source":["### Generator for artifical Dataset.\n","\n","import numpy as np\n","n_samples, n_features = 10, 5\n","rng = np.random.RandomState(0)\n","y = rng.randn(n_samples)\n","X = rng.randn(n_samples, n_features)\n","X[:,4]=2.5*X[:,2]+2.2*X[:,3]+(X[:,4]/100)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E9ghq_geTtzv"},"source":["Retrieve the data and set up the Standard Error (SE) function"]},{"cell_type":"code","metadata":{"id":"UV_jTi-CEaIJ","executionInfo":{"status":"ok","timestamp":1739287176801,"user_tz":0,"elapsed":3,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}}},"source":["import numpy as np\n","n_samples, n_features = 10, 5\n","X=np.array([[ 0.14404357 , 1.45427351,  0.76103773,  0.12167502,  2.17471798],\n","   [ 0.33367433,  1.49407907, -0.20515826,  0.3130677,   0.16731233],\n","   [-2.55298982,  0.6536186,   0.8644362,  -0.74216502,  0.551025  ],\n","   [-1.45436567,  0.04575852, -0.18718385,  1.53277921,  2.91884823],\n","   [ 0.15494743,  0.37816252, -0.88778575, -1.98079647, -6.58069572],\n","   [ 0.15634897,  1.23029068,  1.20237985, -0.38732682,  2.1508076 ],\n","   [-1.04855297, -1.42001794, -1.70627019,  1.9507754,   0.02093387],\n","   [-0.4380743,  -1.25279536,  0.77749036, -1.61389785, -1.60897678],\n","   [-0.89546656,  0.3869025,  -0.51080514, -1.18063218, -3.87468547],\n","   [ 0.42833187,  0.06651722,  0.3024719,  -0.63432209,-0.64295627]])\n","\n","y=[ 1.76405235,0.40015721,  0.97873798,  2.2408932,   1.86755799, -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ]\n","#X2=X[0:10,0:4]\n","#print(X2)\n","def se(X,mse):\n","\n","  SE=np.zeros(len(X[0,:]))\n","  for i in range(0,len(X[0,:])):\n","     SE[i]=np.sqrt(mse/np.square(X[:,i]-np.mean(X[:,i])).sum())\n","\n","  return SE"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CWqVMqhkT2zj"},"source":["We will now implement Lasso regression. You should vary the $\\alpha$ value and see how your paramters change."]},{"cell_type":"code","metadata":{"id":"re8wzj_UEnb2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739287181000,"user_tz":0,"elapsed":1874,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"outputId":"9591ad52-1d1b-4dd8-afa4-621360e9a85c"},"source":["from sklearn.linear_model import Lasso\n","from sklearn.linear_model import LinearRegression\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","import pandas as pd\n","\n","\n","vif = pd.DataFrame()\n","vif[\"VIF Factor\"] = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n","\n","print(vif)\n","\n","print(np.corrcoef(X.transpose()))\n","lasso = Lasso(alpha=0.01,max_iter=10000)\n","lasso.fit(X,y)\n","\n","mse=np.square(y-lasso.predict(X)).sum()/(n_samples-n_features)\n","print('Standard errors are ',se(X,mse))\n","print('r_squared:',lasso.score(X, y))\n","print('reg coefficents : ',lasso.coef_)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["      VIF Factor\n","0       3.553885\n","1       1.265923\n","2  218847.010121\n","3  344541.755303\n","4  369820.450039\n","[[ 1.          0.28495567  0.050991   -0.22445704 -0.17770115]\n"," [ 0.28495567  1.          0.43815271 -0.12449802  0.22222447]\n"," [ 0.050991    0.43815271  1.         -0.35107814  0.44352535]\n"," [-0.22445704 -0.12449802 -0.35107814  1.          0.68349441]\n"," [-0.17770115  0.22222447  0.44352535  0.68349441  1.        ]]\n","Standard errors are  [0.40738126 0.38854141 0.43184097 0.30921653 0.13441583]\n","r_squared: 0.25336565588743165\n","reg coefficents :  [-0.25560658  0.24991356 -0.44210645  0.11113209 -0.        ]\n"]}]},{"cell_type":"markdown","metadata":{"id":"7eV6k4brUmZz"},"source":["The next peice of code is a really simple implementation of Elastic Net. We have balanced the ratio between Ridge and Lasso Regularization at 50:50 (l1_ratio -0.5)."]},{"cell_type":"code","metadata":{"id":"ixZycfHuRt39","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739287213651,"user_tz":0,"elapsed":6,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"outputId":"f7bdb5af-8223-4247-b7c5-b2dd03710034"},"source":["from sklearn.linear_model import ElasticNet\n","regr = ElasticNet(random_state=0,alpha=0.01,l1_ratio=0.5,fit_intercept=True,max_iter=100000)\n","regr.fit(X, y)\n","\n","print('r_squared :',regr.score(X, y))\n","print('reg coefficents : ',regr.coef_)\n"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["r_squared : 0.25377415238976153\n","reg coefficents :  [-0.26193562  0.25858362 -0.44915084  0.11218117 -0.        ]\n"]}]},{"cell_type":"markdown","metadata":{"id":"P2zH1nmuK1SJ"},"source":["#**Review**\n","\n","We can see that is is quite easy to run either the ridge regression or the lasso regression. It is probably best to start your feature selection process with the Lasso regression rather than ridge regression as it has definitive mechanism for dropping a variable. Both require you to choose a value for $\\alpha$, which is a disadvantage. However, using the Elastic Net approach could be a good compromise as ridge regression is better at handling over specified problems.\n","\n","In this step you will see that a number of the parameters have a very wide SE. The Lasso analysis above shows us that 1 variable is set to zero but the rest are not significantly different from zero if you examine the confidence intervals ($\\beta \\pm 1.96.S.E$)\n","\n","Can you explain this? It is a simple answer. Talk about this on the comments board and see if you can come to a consensus.\n","\n","I would also suggest you take the 4 pieces of code from the last 2 steps and create training and test sets. See how your predictions work out with bigger datasets. This [link](https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b) may help you to generate your code.\n","\n","\n"]}]}