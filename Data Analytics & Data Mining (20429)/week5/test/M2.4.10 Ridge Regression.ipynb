{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RW_83RNlllgu"},"source":["#**Ridge Regression (Tikhonov regularization)**\n","\n","In the previous topics we mentioned that it is unwise to include variables in a model that possess multicollinearity. Multicollinearity can create inaccurate estimates of the regression coefficients, inflate the standard errors of the\n","regression coefficients, deflate the partial t-tests for the regression coefficients, give false, nonsignificant, pvalues, and degrade the predictability of the model (and that’s just for starters).\n","\n","There are five sources (see Montgomery [1982] for details):\n","\n","1. Data collection. In this case, the data have been collected from a narrow subspace of the independent variables. The multicollinearity has been created by the sampling methodology—it does not exist in the population. Obtaining more data on an expanded range would cure this multicollinearity problem. The\n","extreme example of this is when you try to fit a line to a single point.\n","\n","2. Physical constraints of the linear model or population. This source of multicollinearity will exist no matter what sampling technique is used. Many manufacturing or service processes have constraints on independent variables (as to their range), either physically, politically, or legally, which will create multicollinearity.\n","\n","3. Over-defined model. Here, there are more variables than observations. This situation should be avoided. NCSS Statistical Software NCSS.com Ridge Regression 335-2 © NCSS, LLC. All Rights Reserved.\n","\n","4. Model choice or specification. This source of multicollinearity comes from using independent variables that are powers or interactions of an original set of variables. It should be noted that if the sampling subspace of independent variables is narrow, then any combination of those variables will increase the\n","multicollinearity problem even further.\n","\n","5. Outliers. Extreme values or outliers in the X-space can cause multicollinearity as well as hide it. We call this outlier-induced multicollinearity. This should be corrected by removing the outliers before ridge regression is applied.\n","\n","\n","The concept behind Ridge regression also know as L2 Regularization is to adjust the estimates that you would normally get from Ordinary Least Squares regression to give new estimates (which have a small amount of bias), but which will have unbiased variance and subsequently deals with inflated VIF and reduce overfitting. It doesn't get rid of attibutes but can point you you to those which are less significant. In ridge regression, the cost function is altered by adding a penalty equivalent to square of the magnitude of the coefficients. It can help us with identifying an dealing with overfitting.\n","\n","In ordinary least squares regression we minimise the following cost function:\n","\n",">>$min_{\\beta}(y-X\\beta)^T(y-X\\beta)$ which gives\n","\n",">>$\\hat{B}_R={(X^TX)}^{-1}X^Ty$\n","\n","In ridge regression we have the following :\n","\n",">>$min_{\\beta}(y-X\\beta)^T(y-X\\beta) +\\lambda(\\beta^T\\beta-c)$ which gives\n","\n",">>$\\hat{B}_R={(X^TX+{\\lambda}I)}^{-1}X^Ty$\n","\n","and is equivalent to saying we are going to minimize the cost function for the Ordinary least squares regression under the condition below:\n","\n",">>For some c> 0 $\\sum_{j=0}^p \\beta^T\\beta<c$\n","\n","Choosing a value for k is not a simple task, which is perhaps one major reason why ridge regression isn’t used as much as least squares or logistic regression. You can read one way to find k in Dorugade and D. N. Kashid’s paper Alternative Method for Choosing Ridge Parameter for Regression. The literature does recommend that the $\\lambda$ value be kept under 0.3.\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HTI5MPWmUax1"},"source":["Now lets complete a ridge regression in python. You will see directly below I have created an artifical dataset which has a large amount of multicollinearity. In the code below and I have printed it out so it relates to my comments below. Only run the generator if you want to experiment\n","I have also written a function to calculate the standard error of the parameter estimates. Statsmodel does this automatically but it is not available for Ridge regression on Google Colabs. We need this to see how the Standard error changes when we implement the ridge regression."]},{"cell_type":"code","metadata":{"id":"FB1Uv3BxQVVw","executionInfo":{"status":"ok","timestamp":1739287027972,"user_tz":0,"elapsed":210,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}}},"source":["### Generator for artifical Dataset.\n","\n","import numpy as np\n","n_samples, n_features = 10, 5\n","rng = np.random.RandomState(0)\n","y = rng.randn(n_samples)\n","X = rng.randn(n_samples, n_features)\n","X[:,4]=2.5*X[:,2]+2.2*X[:,3]+(X[:,4]/100)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"JqvbXEpTZd0y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739287029666,"user_tz":0,"elapsed":13,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"outputId":"23a01fe6-a144-403b-aa07-56849263d5f9"},"source":["import numpy as np\n","n_samples, n_features = 10, 5\n","X=np.array([[ 0.14404357 , 1.45427351,  0.76103773,  0.12167502,  2.17471798],\n","   [ 0.33367433,  1.49407907, -0.20515826,  0.3130677,   0.16731233],\n","   [-2.55298982,  0.6536186,   0.8644362,  -0.74216502,  0.551025  ],\n","   [-1.45436567,  0.04575852, -0.18718385,  1.53277921,  2.91884823],\n","   [ 0.15494743,  0.37816252, -0.88778575, -1.98079647, -6.58069572],\n","   [ 0.15634897,  1.23029068,  1.20237985, -0.38732682,  2.1508076 ],\n","   [-1.04855297, -1.42001794, -1.70627019,  1.9507754,   0.02093387],\n","   [-0.4380743,  -1.25279536,  0.77749036, -1.61389785, -1.60897678],\n","   [-0.89546656,  0.3869025,  -0.51080514, -1.18063218, -3.87468547],\n","   [ 0.42833187,  0.06651722,  0.3024719,  -0.63432209,-0.64295627]])\n","X2=X[0:10,0:4]\n","print(X2)\n","\n","y=[ 1.76405235,0.40015721,  0.97873798,  2.2408932,   1.86755799, -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ]"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0.14404357  1.45427351  0.76103773  0.12167502]\n"," [ 0.33367433  1.49407907 -0.20515826  0.3130677 ]\n"," [-2.55298982  0.6536186   0.8644362  -0.74216502]\n"," [-1.45436567  0.04575852 -0.18718385  1.53277921]\n"," [ 0.15494743  0.37816252 -0.88778575 -1.98079647]\n"," [ 0.15634897  1.23029068  1.20237985 -0.38732682]\n"," [-1.04855297 -1.42001794 -1.70627019  1.9507754 ]\n"," [-0.4380743  -1.25279536  0.77749036 -1.61389785]\n"," [-0.89546656  0.3869025  -0.51080514 -1.18063218]\n"," [ 0.42833187  0.06651722  0.3024719  -0.63432209]]\n"]}]},{"cell_type":"code","metadata":{"id":"iGrVEkQDMDc_","executionInfo":{"status":"ok","timestamp":1739287033328,"user_tz":0,"elapsed":8,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}}},"source":["def se(X,mse):\n","\n","  SE=np.zeros(len(X[0,:]))\n","  for i in range(0,len(X[0,:])):\n","     SE[i]=np.sqrt(mse/np.square(X[:,i]-np.mean(X[:,i])).sum())\n","\n","  return SE"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cybrHm_kWqAb"},"source":["The following piece of code prints out the correlation matrix and the VIF for the 5 X factors. It then goes on to produce the Coefficients, RSquared and the standard errors for each parameter. If you multiply the Standard error by 1.96 and subtract and add this value to your parameter estimate. You should get a confidence interval which is\n","\n",">>$\\beta \\pm 1.96.S.E$\n","\n","If zero falls in this interval then it tells us the parameter in question should be dropped."]},{"cell_type":"code","metadata":{"id":"VhmZunqjxnFv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739287035009,"user_tz":0,"elapsed":11,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"outputId":"6ac8122a-6a22-4932-e7b3-30cf8283cf74"},"source":["from sklearn.linear_model import LinearRegression\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","import pandas as pd\n","\n","vif = pd.DataFrame()\n","vif[\"VIF Factor\"] = [variance_inflation_factor(X2, i) for i in range(X2.shape[1])]\n","\n","print(vif)\n","\n","print(np.corrcoef(X.transpose()))\n","reg = LinearRegression().fit(X2, y)\n","mse=np.square(y-reg.predict(X2)).sum()/(n_samples-(n_features))\n","print('Standard errors are ',se(X2,mse))\n","print('r_squared :',reg.score(X2, y))\n","print('reg coefficents : ',reg.coef_)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["   VIF Factor\n","0    1.015238\n","1    1.239427\n","2    1.362308\n","3    1.151187\n","[[ 1.          0.28495567  0.050991   -0.22445704 -0.17770115]\n"," [ 0.28495567  1.          0.43815271 -0.12449802  0.22222447]\n"," [ 0.050991    0.43815271  1.         -0.35107814  0.44352535]\n"," [-0.22445704 -0.12449802 -0.35107814  1.          0.68349441]\n"," [-0.17770115  0.22222447  0.44352535  0.68349441  1.        ]]\n","Standard errors are  [0.40718094 0.38835036 0.43162862 0.30906448]\n","r_squared : 0.25409974966549564\n","reg coefficents :  [-0.27391759  0.27547156 -0.4669002   0.11115832]\n"]}]},{"cell_type":"code","metadata":{"id":"gdkeFS9RF2cx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739287038497,"user_tz":0,"elapsed":6,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"outputId":"b5e65106-39ac-4b4d-8fe3-b93ac3f9c696"},"source":["print(y)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[1.76405235, 0.40015721, 0.97873798, 2.2408932, 1.86755799, -0.97727788, 0.95008842, -0.15135721, -0.10321885, 0.4105985]\n"]}]},{"cell_type":"markdown","metadata":{"id":"tRzGHBKYYGEt"},"source":["We can see using the formula in the previous section that the first 2 regression coefficents are in significant. The remianing 3 are higly significant and would possibly suggest multi-collinearity or an over specified model."]},{"cell_type":"code","metadata":{"id":"t55wFo5BxWF7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739287040924,"user_tz":0,"elapsed":10,"user":{"displayName":"Andrew Mccarren","userId":"16186536572019350587"}},"outputId":"283d8208-9129-4867-f344-9679871baa65"},"source":["from sklearn.linear_model import Ridge\n","#X=X[:,0:4]\n","clf = Ridge(alpha=0.2)\n","ridge=clf.fit(X, y)\n","\n","mse=np.square(y-ridge.predict(X)).sum()/(n_samples-n_features)\n","print('Standard errors are ',se(X,mse))\n","print('r_squared :',ridge.score(X,y))\n","print('reg coefficents : ',ridge.coef_)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Standard errors are  [0.4073432  0.38850512 0.43180063 0.30918765 0.13440328]\n","r_squared : 0.2535051418548737\n","reg coefficents :  [-0.26452402  0.26160454 -0.29887643  0.24664966 -0.06131185]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Drljw6nVY38J"},"source":["If we use a $\\lambda$ of 0.2 we get the last variable being the only significant variable. This doesnt makes sense as we created it from X3 and X4 and we would have expected the 5th variable to be insignificant and thus should be dropped. However, if you remember back in MOOC 1 we talked about being very careful when removing variables. It is not straight forward. You will see that when we do Lasso Regression that the 5th variable is the one selected for exclusion."]},{"cell_type":"markdown","metadata":{"id":"TooorSklb9wL"},"source":["#**Review**\n","\n","We have looked at how to implement Ridge Regression and how the multicollinearity can send us in the wrong direction with respect to the assumptions about which variables are appropriate.The major drawback to it is that there is no selection methodology for $\\lambda$.\n","\n","Adjust the values of alpha ($\\lambda$) and see what happens.\n","\n","We are now going to move to the next step which will cover Lasso Regression."]}]}