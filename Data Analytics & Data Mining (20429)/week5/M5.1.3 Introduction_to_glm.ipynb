{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M5.1.3 Introduction_to_glm.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cRlLIOXnjZh7","colab_type":"text"},"source":["\n","#**Introduction to the general linear model**\n","\n","In step 1.3.9 we introduced the concept of a linear regression model. The model typically looks like \n","\n","$$Y_i=\\alpha + \\beta_i \\times X_i + \\epsilon_i$$\n","\n","where $\\epsilon_i$ is a random normal distribution with $\\mu=0$ and $\\sigma$ is constant ~ $N(0,\\sigma)$\n","\n","This assumption of normallity of the error with a constant variannce for a continous variable $Y_i$ is important as when it is meet it allows us to use this approach to assess the impact that the predictor variable has. If it is not meet we cannot really on the $\\beta$ estimates to inform us of the contribution of each predictor variable to the outcome variable $Y$. This, we cannot use inferential statistics to estimate if the variable in question has a significant impact on the outcome.\n","\n","Linear Regression models have five key assumptions, if we want to use inferential statistics to determine the significance of each variable and they are as follows:\n","\n","* Linear relationship exists between predictor and outcome variables\n","* Normality of predictor variable and error distribution\n","* No or little multicollinearity (predictors are not correlated with each other)\n","* No auto-correlation (errors are independent)\n","* Homoscedasticity (error variance is constant)\n","\n","In Linear regression a rule of thumb is that we require at least 20 cases per independent variable in the analysis.\n","\n","\n","If these assumptions are meet in theory the estimates of $\\beta$ of the regression model will be deemed as \"BLUE\"  or best linear unbiased estimates. This means that the we can in theory estimate the predictive range of our outcome variable. \n","\n","One of the problems with the above model is that we are not always using continous normal predictor variables. In fact the predictor variable could be a classifier $\\{0,1\\}$, a count variable  or a variable that has censored values for example.\n","\n","This is where Generalised Linear models comes in. This is a class of models that are based on the simple linear regression but have the ability to handle a range of outcome variables that come from the exponential family of variables. Some of the distributions that the outcome variable can fall into are the following:\n","\n","* Gaussian regression\n","* Poisson regression\n","* Binomial regression (classification)\n","* Multinomial classification\n","* Gamma regression\n","* Ordinal regression\n","* Negative Binomial regression\n","\n","GLM works by using a \"link function\" to transform the outcome variable to a variable that can be linearly regressed with the predictor variables:\n","\n","$$g(y)=\\alpha + \\beta_1 X_1 +\\beta_1 X_1 + \\beta_1 X_1$$\n","\n","So in theory we can transform the various exponential variable and apply linear regression to understand the impact of the predictor variables on the outcome variable. Over the comming steps we will look at various forms of the GLM. The following [youtube](https://www.youtube.com/watch?v=G5xIFdLL5Ic) video also gives a good eplaination of them.\n","\n","As usual leave your thoughts on the comments board.\n","\n","\n","\n","\n","\n"]}]}