{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pxdU4CXP_8fq"},"source":["#**Multicollinearity**\n","\n","In steps 1.3.12 and 2.4.9 we dicsussed Correlation and Multi-Collinearity. Multi-collinearity can be defined as a phenomenon in which one predictor variable in a model can be linearly predicted from the remaining variables. I appreciate that most of you will feel we are repeating ourselves. However, the impact and conclusions drawn from  proposed algorithms can be extremely important if Multi-Collinearity exists in the data. Now many of you might recap what was said in step 1.3.12 about the effect of Multicollinearity can have on our models. In linear regression it potentially infaltes the standard error of some of the parameter estimates. When this happens we are inclined to drop variables that are relevant and keep irrelevant variables. Many of you will say why is this such a big deal, as all we are interested in is making predictions. In theory this is true. Multi-collinearty doesn't effect the predictions. But understanding which variables are relevant to our analysis is extremely important. The following reasons are why we should deal with it:\n","\n","> Under perfect multicollinearity, Ordinarly Least Squares (OLS) estimates simply don't exist. In OLS the solution we are trying to find the best estimates of $\\beta$  to the following equation:\n","\n",">>$y=\\beta X+\\epsilon$ where $\\epsilon$ ~ $N(o,\\sigma)$\n","\n",">The solution to this problem can be shown to be:\n","\n",">>$\\hat{\\beta}$ = $(X^TX)^{-1}X^Ty$\n","\n",">Now note that we have $(X^TX)^{-1}$ in this equation. This means that we have to be able to invert the $(X^TX)$. If any of the independent variables are perfectly collinear with others, then  this matrix will not be invertable as the rank of the matrix will be less than the number of dimensions+1.\n","\n","> Now many of you might think that the variables will not be perfectly correlated, and you might be right here. Well if there is a \"strong\" but not perfect colinearity between variables then you probably will find that your model will be quite sensitive to minor changes in the data. This is caused by ill conditioning and many libaries will strugle to converge. Even if you get your library to work it will most likely come up with the volatile weights which will cause errors with regard to variable importance.\n","\n","> There will be those amongst you who will think that you are going to use a non-paramteric model such as a neural network and this issue won't matter. Well I am sorry to burst your bubble, but you will still have issues, because neural networks use algorithms such as steppest descent to optimize the cost function. These algorithms struggle to optimize when there are non orthogonal features in the cost function (perfect orthogonality effectively means we have no multi-collinearity  in our input features/weights). The net effect is we may not converge or in some cases we will end up in a nonlinear \"hole\", thus we are less likely to find the global optimum.\n","\n","\n","Finally, you will come across a lot of material about multi-collinearity on-line. From a data mining perspective it is very important to understand the amount of multi-collinearity in our data as it will allow us to understand which variables are important in our models and to determine the impact that these variables have with our outcome variables. One should note that in clinical trials and loan approval models authorities such as the FDA or central banks insist on understanding the impact of each variable.\n","\n","\n","Now were are going to implement the VIF in the next step using the Python \"statsmodels\" library.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]}