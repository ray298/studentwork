{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M4.1.8 MS Apriori.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0HI0RxCGzaLz","colab_type":"text"},"source":["#MS-Apriori\n","The Apriori algorithm that we looked at in MOOC4.1.6 assumed all items had the same minimum support and that all items have a similar frequency. In reality this is not the case as some items appear very frequently in our data, while others rarely appear. For example in a suppermarket, people buy food processors and cooking pans much less frequently than they buy bread and milk.\n","\n","\n","If the frequencies of items vary a great deal, we will encounter two problems\n","\n","*   If minsup is set too high, those rules that involve rare items will not be \n","found. \n","*   To find rules that involve both frequent and rare items, minsup has to be set very low. This may cause combinatorial explosion because those frequent items will be associated with one another in all possible ways. \n","\n","You may ask yourself why would you care if something is a rare item. Well if any of you shop in Lidil or Aldi then you will have noticed that they  promotions on rare items such as tents or wet suits. These supermarkets know what items will be sold with these items and this knowledge allows them to plan their marketing strategy. People won't buy these things every week but the supermarket knows that if they do a promotion on these items then associated items will be sold as well. \n","\n","So the answer to this problem is to have multiple minimum supports. The minimum support of a rule is expressed in terms of minimum item supports (MIS) of the items that appear in the rule. Each item can have a minimum item support, and by providing different MIS values for different items, the user effectively expresses different support requirements for different rules. \n","\n","So lets go a little deeper:\n","\n","Let $MIS(a_i)$ be the the MIS value if item $i$. The minsup of a rule $R$ is the lowest value of the items in the rule.\n","\n","In other words a rule \n","\n","$R: a_1,a_2,..,a_k \\mapsto a_{k+1},...,a_r$ satisfies its minimum support if its actual support is $\\ge$  $min(MIS(a_1),MIS(a_2),...,MIS(a_r))\n","\n","\n","Lets look at an example:\n","\n","Consider the following items:\n","\n","$ [bread, shoes, clothes ] $\n","\n","The user-specified MIS values are as follows:\n","$MIS(bread) = 2\\%$, $MIS(shoes) = 0.1\\%$, $MIS(bread) = 0.2\\%$\n","\n","The following rule doesnâ€™t satisfy its minsup:\n","\n",">> $clothes \\mapsto bread$  $[ sup=0.15\\%,conf =70\\%]$\n","\n","Why? well the support for $clothes \\mapsto bread$ is $15\\%$ which is less than the $Min(MIS(bread),MIS(bread) $ = $Min(2\\%,0.2\\%)$\n","\n",">> $clothes \\mapsto shoes$  $[ sup=0.15\\%,conf =70\\%]$ \n","\n","Why? well the support for $clothes \\mapsto shoes$ is $15\\%$ which is greater than the $Min(MIS(bread),MIS(shoes) $ = $Min(2\\%,0.1\\%)$\n","\n","#**Benifits of MS-Apriori**\n",">Multiple minsup model subsumes the single support model.\n",">It is a more realistic model for practical applications.\n",">The model enables us to found rare item rules yet without producing a huge number of meaningless rules with frequent items.\n",">By setting MIS values of some items to 100% (or more), we effectively instruct the algorithms not to generate rules only involving these items.  \n","\n","\n","\n","I am not going to describe the algorithm in detail, however, for those that are interested you can go [here](https://www.computing.dcu.ie/~amccarren/mcm_slides/Lecture_6_association_rules.ppt) and see my slides for Association Rule Mining.\n","\n","Now lets have a look at working example in Python. \n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}]}